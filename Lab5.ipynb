{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, min_length = 3, lower = False):\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "    spec = re.compile(r\"[^A-Za-z0-9]\\s\")\n",
    "    pattern = re.compile(r'^\\w*[-\\']?\\w*$')\n",
    "    temp = re.sub(spec,\"\", text)\n",
    "    text_sep = [tk for tk in list(filter(pattern.match,text.split(\" \"))) if len(tk) > min_length]\n",
    "    if lower:\n",
    "        return list(set([tk.lower() for tk in text_sep]))\n",
    "        \n",
    "    else:\n",
    "        return list(set(text_sep))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(posting1 , posting2):\n",
    "    posting_intersect = []\n",
    "    n1 = len(posting1)\n",
    "    n2 = len(posting2)\n",
    "    \n",
    "    pt1 = 0\n",
    "    pt2 = 0\n",
    "    \n",
    "    while(pt1 < n1 and pt2 < n2):\n",
    "        \n",
    "        if posting1[pt1] == posting2[pt2]:\n",
    "            posting_intersect.append(posting1[pt1])\n",
    "            pt1 = pt1 + 1\n",
    "            pt2 = pt2 + 1\n",
    "            \n",
    "        elif posting1[pt1] <posting2[pt2] :\n",
    "            pt1 = pt1 + 1;\n",
    "        \n",
    "        elif posting1[pt1] > posting2[pt2]:\n",
    "            pt2 = pt2 + 1;\n",
    "    \n",
    "    return posting_intersect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_intersect(postings):\n",
    "    postings.sort(key = len)\n",
    "    \n",
    "    post_len = len(postings)\n",
    "    temp = postings[0]\n",
    "    \n",
    "    for i in range(1,post_len):\n",
    "        temp = intersect(temp,postings[i])\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serious very', 'very condition']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_phrase_query(query):\n",
    "    tokens = tokenize_text(query,3,True)\n",
    "    bigrams = []\n",
    "    for tk in range(0,len(tokens)-1):\n",
    "        bigrams.append(tokens[tk] + \" \"+   tokens[tk+1])\n",
    "        \n",
    "    return bigrams\n",
    "\n",
    "parse_phrase_query(\"very serious condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = \"C:\\\\Users\\\\acer\\\\Desktop\\\\jamal\\\\bbc\\\\sport\\\\\"\n",
    "files = os.listdir(path_to_corpus)\n",
    "\n",
    "tokens_p_file = []\n",
    "post = {}\n",
    "\n",
    "for file in files:\n",
    "    textfile = open(path_to_corpus + file,'r').read()\n",
    "    unigrams = tokenize_text(textfile,3,True)\n",
    "    bigrams = []\n",
    "    for tk in range(0,len(unigrams)-1):\n",
    "        bigrams.append(unigrams[tk] + \" \"+   unigrams[tk+1])\n",
    "        \n",
    "    for tk in unigrams:\n",
    "        if post.get(tk) == None:\n",
    "            post[tk] = [file[:-5]]\n",
    "        else:\n",
    "            post[tk].append(file[:-5])\n",
    "    \n",
    "    for tk in bigrams:\n",
    "        if post.get(tk) == None:\n",
    "            post[tk] = [file[:-5]]\n",
    "        else:\n",
    "            post[tk].append(file[:-5])   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ars', 'ornage', 'zpple']\n"
     ]
    }
   ],
   "source": [
    "lt = ['zpple', 'ornage', 'ars']\n",
    "lt.sort()\n",
    "print(lt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
